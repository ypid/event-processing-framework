---
# vim: foldmarker=[[[,]]]:foldmethod=marker

# SPDX-FileCopyrightText: 2021 Robin Schneider <robin.schneider@geberit.com>
#
# SPDX-License-Identifier: AGPL-3.0-only

transforms:
  transform_prepare_event_metadata:
    type: remap
    inputs:
      - 'transform_source_*'
      # - 'source_vector_*'
      # - 'transform_source_enable_preprocessor_syslog'
    source: |-
      # Drop not needed field that is set by file sources.
      del(.log.file.path)

      # Capture other fields that Vector.dev added.
      __timestamp = del(."@timestamp")
      __source_type = del(.source_type)

      if (!exists(.__.event.original)) {
        .__.event.original = .
        del(.__.event.original.__)
      }
      if ((get_env_var("TEST_MODE") ?? false) == "true") {
        if (exists(.__.event.original.host)) {
          del(.__.event.original.host.name)
          if (length(.__.event.original.host) == 0 ?? false) {
            del(.__.event.original.host)
          }
        }
      }

      .__."@timestamp" = __timestamp
      .__.source_type = __source_type

      .__.parse_failures_short = []
      .__.parse_failures = []
      .__.parse_warnings_short = []
      .__.parse_warnings = []

      .__.fingerprint_inputs = []

      # This must be set by a transform attached to each sink instead of
      # setting it here.
      # .__.enabled_preprocessors = {}

      .__.enabled_postprocessors = {
        "file path sanitation": {
          "log.file.path": true,
        },
        "host name sanitation": {
          "host.name": true,
          "observer.host.name": true,
        },
        "host.name missing": {
        },
        "host name QA": {
          "host.name": true,
          "observer.host.name": true,
        },
        "timestamp QA": {
          # Upper variation of duration until which the timestamp is considered
          # valid compared to event.created.
          # Boolean false disables QA of a field.
          "@timestamp": "10 m",
        },
      }

      # Use @timestamp set by Vector source as event.created but allow to refer
      # to .__."@timestamp" later (do not delete it here).
      if !exists(.event.created) {
        .event.created = .__."@timestamp"
      }

      # Handle Kafka transport [[[
      if (.__.source_type == "kafka") {
        if (is_integer(.partition) && is_integer(.offset)) {
          # TODO: Consider moving this code to transform_postprocess (after
          # modules) to only do it event.sequence was not set by module for
          # performance reasons?
          partition = int!(.partition)
          if (partition > 9_222) {
            partition = mod(partition, 9_223)
            .__.parse_warnings_short = push(.__.parse_warnings_short, "parse_warning: kafka")
            .__.parse_warnings = push(.__.parse_warnings, "parse_warning: kafka. Found partition higher than 9222 when calculating event.sequence. So many partitions does not seem right. The value was capped to prevent integer overflow.")
          }
          offset = int!(.offset)
          if (offset > 999_999_999_999_999) {
            offset = mod(offset, 1_000_000_000_000_000)
            .__.parse_warnings_short = push(.__.parse_warnings_short, "parse_warning: kafka")
            .__.parse_warnings = push(.__.parse_warnings, "parse_warning: kafka. Found offset higher than 999,999,999,999,999 when calculating event.sequence. The value was capped to prevent integer overflow.")
          }
          .event.sequence = partition * 1_000_000_000_000_000 + offset
          # Multiply partitions by
          #     1_000_000_000_000_000
          # so that partition + offset is encoded in the sequence.
          # 9_223_372_036_854_775_807 is the highest possible integer value in Vector.
          # Ref: https://vector.dev/docs/reference/vrl/expressions/#integer
          # This gives room for 9223 partitions per topic.
        } else {
          .__.parse_warnings_short = push(.__.parse_warnings_short, "parse_warning: kafka")
          .__.parse_warnings = push(.__.parse_warnings, "parse_warning: kafka. The metadata offset and partition added by the Vector Kafka source component are not intergers. Cannot set .event.sequence")
        }
        .__.source.kafka.headers = del(.headers)
        .__.source.kafka.message_key = del(.message_key)
        .__.source.kafka.partition = del(.partition)
        .__.source.kafka.offset = del(.offset)
        .__.source.kafka.topic = del(.topic)

        .__.event.original = .message
      }
      # ]]]

      # TODO: We should default to the timezone of the site/location of the device.
      # To implement this, lookup tables are needed.
      #
      # This does not work however: parse_timestamp!(join!([parsed_csv_object.origin_timestamp, "CEST"], " "), format: "%d.%m.%Y %H:%M:%S %Z")
      # because a timestamp with CEST is not unambiguous because it means both +01:00 and +02:00 depending on DST.

      if (bool(.__.enabled_preprocessors."decode outer json") ?? false) {
        parsed_outer_json, err = parse_json(.message)
        if err != null {
          .__.parse_failures_short = push(.__.parse_failures_short, "parse_failure: outer json")
          .__.parse_failures = push(.__.parse_failures, "parse_failure: outer json. " + err)
        } else {
          if (bool(.__.trusted_input) ?? false) {
            # TODO: Remove this unneeded condition.
            . = merge!(., parsed_outer_json, deep: true)
          } else {
            # Preserve fields from parsed_outer_json because fields from `.`
            # take precedence in the following `merge`.
            if (exists(parsed_outer_json.event.sequence)) {
              .__.event.original_parsed_outer_json.event.sequence = parsed_outer_json.event.sequence
            }
            . = merge!(parsed_outer_json, ., deep: true)
            .message = parsed_outer_json.message
          }
          if (exists(parsed_outer_json.host.name)) {
            .host.name = parsed_outer_json.host.name
          }
          if exists(parsed_outer_json.event.created) {
            .event.created = parsed_outer_json.event.created
          }

          # Filebeat uses unspecified (in ECS) field log.offset instead.
          if (exists(.log.file.offset)) {
            .event.sequence = del(.log.file.offset)
          }

          # Drop inputname field that is populated by Rsyslog but not meant
          # to end up in Elastic.
          del(.inputname)

          # Drop parsed syslog fields that are populated by Rsyslog.
          # Syslog will be reparsed by Vector if the log format is indeed
          # syslog.
          del(.log.syslog)
          del(.log.facility)

          # Decode optional JSON in JSON [[[
          # This is the case JSON is sent to Rsyslog.
          if (((bool(.__.enabled_preprocessors."decode inner json") ?? false) && starts_with(.message, "{")) ?? false) {
            parsed_inner_json, err = parse_json(.message)
            if err != null {
              .__.parse_failures_short = push!(.__.parse_failures_short, "parse_failure: inner json")
              .__.parse_failures = push!(.__.parse_failures, "parse_failure: inner json. " + err)
            } else {
              # Drop fields which were populated Rsyslog by trying to parse
              # the JSON as syslog and then end up with default values
              # because JSON in not valid syslog.
              del(.log)
              del(."@timestamp")

              .__.event.original = .message

              # We take more care unpacking the inner JSON because it is
              # untrusted. Other than the JSON that Rsyslog encoded which we
              # trust.
              # This is done by giving fields from the outer JSON a higher
              # prio so they will overwrite fields in the inner JSON and then
              # selectively picking fields from the inner JSON to overwrite
              # once in the outer JSON.
              . = merge!(parsed_inner_json, ., deep: true)
              if (exists(parsed_inner_json.host.name)) {
                .host.name = parsed_inner_json.host.name
              }
              .message = parsed_inner_json.message
            }
          }
          # ]]]

        }
      }

      # Remove Microsoft newline \r legacy if existing.
      .message = replace(.message, r'\r$', "") ?? .message

  transform_preprocessors_last:
    type: remap
    inputs:
      - 'transform_hook_post_json_parsing'
    source: |-
      if (!exists(.__.parse_failures)) {
        # Component called from isolated unit test.
        .__.parse_failures_short = []
        .__.parse_failures = []
        .__.parse_warnings_short = []
        .__.parse_warnings = []
        .__.fingerprint_inputs = []
      }

      if (bool(.__.enabled_preprocessors.journald) ?? false) {
        .event.original = .__.event.original
        .__.fingerprint_inputs = push!(.__.fingerprint_inputs, .event.original)

        # The Vector journald source component has converted this to @timestamp
        # for us already.
        del(._SOURCE_REALTIME_TIMESTAMP)

        del(.__MONOTONIC_TIMESTAMP)
        del(.__REALTIME_TIMESTAMP)

        # Not specified by ECS [[[
        # The fields in this code fold (except "process" and "event" field set)
        # are not specified as of ECS 8.8. Field names are from
        # https://docs.elastic.co/integrations/journald
        if (exists(.SYSLOG_IDENTIFIER)) { .log.syslog.identifier = del(.SYSLOG_IDENTIFIER) }
        if (exists(._CAP_EFFECTIVE)) { .journald.process.capabilities = del(._CAP_EFFECTIVE) }
        if (exists(._CMDLINE)) { .process.command_line = del(._CMDLINE) }
        if (exists(._COMM)) { .process.name = del(._COMM) }
        if (exists(._SYSTEMD_CGROUP)) { .systemd.cgroup = del(._SYSTEMD_CGROUP) }
        if (exists(._SYSTEMD_INVOCATION_ID)) { .systemd.invocation_id = del(._SYSTEMD_INVOCATION_ID) }
        if (exists(._SYSTEMD_SLICE)) { .systemd.slice = del(._SYSTEMD_SLICE) }
        if (exists(._SYSTEMD_UNIT)) { .systemd.unit = del(._SYSTEMD_UNIT) }
        if (exists(._TRANSPORT)) { .systemd.transport = del(._TRANSPORT) }
        int_field_specs_to_parse = [
          {"source": ["PRIORITY"],  "destination": ["event", "severity"]},
          {"source": ["_GID"],  "destination": ["journald", "gid"]},
          {"source": ["_PID"],  "destination": ["process", "pid"]},
          {"source": ["_UID"],  "destination": ["journald", "uid"]},
        ]
        for_each(int_field_specs_to_parse) -> |_index, int_field_spec| {
          if (get!(., path: int_field_spec.source) != null) {
            parsed_value, err = parse_int(string!(get!(., path: int_field_spec.source)))
            if err != null {
              .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: journald")
              .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: journald. Drop non-int field " + join!(int_field_spec.source, separator: ".") + ": " + err + ". String: " + string!(get!(., path: int_field_spec.source)))
            } else {
              . = set!(value: ., path: int_field_spec.destination, data: parsed_value)
            }
            . = remove!(., path: int_field_spec.source)
          }
        }
        # ]]]

        if (exists(.SYSLOG_FACILITY)) {
          .log.syslog.facility.code, err = parse_int(.SYSLOG_FACILITY)
          if err != null {
            .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: syslog")
            .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: syslog. Drop non-int field facility field. " + err)
            del(.log.syslog.facility.code)
          } else {
            .log.syslog.facility.name, err = to_syslog_facility(.log.syslog.facility.code)
            if err != null {
              .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: syslog")
              .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: syslog. Could not convert facility code to name. " + err)
              del(.log.syslog.facility.name)
            }
          }
          del(.SYSLOG_FACILITY)
        }

        del(.SYSLOG_TIMESTAMP)
        del(.SYSLOG_PID)

        if (exists(._MACHINE_ID)) { .host.id = del(._MACHINE_ID) }
        if (exists(._BOOT_ID)) {    .host.boot.id = del(._BOOT_ID) }

        .journald.metadata = {}
        for_each(keys(.)) -> |_index, root_key| {
          if (starts_with(root_key, "_") && root_key != "__") {
            .journald.metadata = set!(value: .journald.metadata, path: [root_key], data: get!(., path: [root_key]))
            . = remove!(., path: [root_key])
          }
        }
      }
      if (bool(.__.enabled_preprocessors.kubernetes) ?? false) {
        # De-dot all keys. Also done by Filebeat and Elastic.
        # Ref: https://github.com/elastic/integrations/blob/main/packages/kubernetes/data_stream/container_logs/sample_event.json
        # https://github.com/vectordotdev/vector/issues/19136
        # TODO: Only de-dot keys that need it? Or better check if a index
        # mapping can be defined that does not require this sanitation at all.
        .kubernetes = map_keys(object!(.kubernetes), recursive: true) -> |key| { replace(key, ".", "_") }

        if (.stream == "stderr") {
          .log.level = "error"
          .event.severity = 3
        } else {
          .log.level = "info"
          .event.severity = 6
        }
        .kubernetes.stream = del(.stream)
        .log.file.path = del(.file)
      }

      if (bool(.__.enabled_preprocessors.syslog) ?? false) {
        parsed_syslog, err = parse_syslog(.message)
        if err != null {
          if (!(bool(.__.enabled_preprocessors.syslog_lax) ?? false)) {
            .__.parse_failures_short = push!(.__.parse_failures_short, "parse_failure: syslog")
            .__.parse_failures = push!(.__.parse_failures, "parse_failure: syslog. " + err)
          }
        } else {
          del(.log.syslog)
          del(.log.facility)

          .event.original = .message
          .__.fingerprint_inputs = push!(.__.fingerprint_inputs, .event.original)

          if (is_integer(parsed_syslog.version)) {
            .log.syslog.version = parsed_syslog.version
          }
          del(parsed_syslog.version)

          .host.name = del(parsed_syslog.hostname)
          .message = del(parsed_syslog.message)

          # Syslog priority (facility and severity) [[[
          .log.level = del(parsed_syslog.severity)
          .log.syslog.facility.name = del(parsed_syslog.facility)
          .event.severity, err = to_syslog_severity(.log.level)
          if err != null {
              warning_message_short = "parse_warning: syslog severity"
              .__.parse_warnings_short = push!(.__.parse_warnings_short, warning_message_short)
              .__.parse_warnings = push!(.__.parse_warnings, warning_message_short + ": " + err)
            del(.event.severity)
          }
          # ]]]

          if (exists(parsed_syslog.appname)) {
            .log.logger = del(parsed_syslog.appname)
            if (is_nullish(.log.logger)) { del(.log.logger) }
          }
          if (exists(parsed_syslog.procid)) {
            # RFC 5424: The PROCID field is often used to provide the process
            # name or process ID associated with a syslog system.
            if (is_integer(parsed_syslog.procid)) {
              .process.pid = parsed_syslog.procid
            } else if (is_string(parsed_syslog.procid)) {
              .process.name = parsed_syslog.procid
            } else if (!is_nullish(parsed_syslog.procid)) {
              .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: procid invalid")
              .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: procid invalid. Syslog procid is neither int nor string. Drop value: " + string!(.process.pid))
            }
            del(parsed_syslog.procid)
          }
          if (exists(parsed_syslog.msgid)) {
            .event.code = del(parsed_syslog.msgid)
            if (is_nullish(.event.code)) { del(.event.code) }
          }

          int_field_specs_to_parse = [
            {"source": ["meta", "sysUpTime"],  "destination": ["host", "uptime"]},
            {"source": ["meta", "sequenceId"], "destination": ["event", "sequence"]},
          ]
          for_each(int_field_specs_to_parse) -> |_index, int_field_spec| {
            if (get!(parsed_syslog, path: int_field_spec.source) != null) {
              parsed_value, err = parse_int(string!(get!(parsed_syslog, path: int_field_spec.source)))
              if err != null {
                .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: syslog")
                .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: syslog. Drop non-int field " + join!(int_field_spec.source, separator: ".") + ": " + err + ". String: " + string!(get!(parsed_syslog, path: int_field_spec.source)))
              } else {
                if (int_field_spec.destination == ["host", "uptime"]) {
                  parsed_value = to_int(round(parsed_value / 100))
                }
                . = set!(value: ., path: int_field_spec.destination, data: parsed_value)
              }
              parsed_syslog = remove!(parsed_syslog, path: int_field_spec.source)
            }
          }

          if (is_object(parsed_syslog.meta) && length(object!(parsed_syslog.meta)) == 0) {
            del(parsed_syslog.meta)
          }

          ."@timestamp" = del(parsed_syslog.timestamp)
          if (length(parsed_syslog) > 0) {
            . = set(., path: (split(.event.dataset, ".") ?? ["other", "other"]), data: parsed_syslog) ?? .
          }
          .__.preprocessor_success.syslog = true
        }
      }

      if (
          !(exists(.__.preprocessor_success.syslog) && bool(.__.preprocessor_success.syslog)) &&
          bool(.__.enabled_preprocessors.syslog_minimal) ?? false
      ) {
        .event.original = .message
        if (exists(.__)) {
          .__.fingerprint_inputs = push!(.__.fingerprint_inputs, .event.original)
        }

        parsed_grok, err = parse_grok(.message, "^<%{NONNEGINT:log.syslog.priority}>\\s*%{GREEDYDATA:message}")

        if err != null {
          if (!(bool(.__.enabled_preprocessors.syslog_lax) ?? false)) {
            .__.parse_failures_short = push!(.__.parse_failures_short, "parse_failure: grok syslog prio")
            .__.parse_failures = push!(.__.parse_failures, "parse_failure: grok syslog prio. Tried to extract syslog prio from log line: " + err)
          }
        } else {
          # Syslog priority parsing [[[
          decode_priority = "<" + parsed_grok."log.syslog.priority" + ">1 2021-01-01T00:00:00.000Z  - - - - []" ?? ""

          parsed_priority, err = parse_syslog(decode_priority)

          if err !=  null {
            if (!(bool(.__.enabled_preprocessors.syslog_lax) ?? false)) {
              .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: syslog priority")
              .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: syslog priority. Could not parse syslog priority.")
            }
          } else {
            # Syslog priority (facility and severity) [[[
            .log.level = del(parsed_priority.severity)
            .log.syslog.facility.name = del(parsed_priority.facility)
            .event.severity, err = to_syslog_severity(.log.level)
            if err != null {
              warning_message_short = "parse_warning: syslog severity"
              .__.parse_warnings_short = push!(.__.parse_warnings_short, warning_message_short)
              .__.parse_warnings = push!(.__.parse_warnings, warning_message_short + ": " + err)
              del(.event.severity)
            } else {
              .message = parsed_grok.message
              .__.preprocessor_success.syslog_minimal = true
            }
            # ]]]
          }
          # ]]]
        }
      }

      # Fake high precision timestamp to preserve log order. [[[
      #
      # Does not work as it does not carry the initial precision anymore.
      # ts_fract_secs, err = format_timestamp(.timestamp, "%f")
      #
      # After reconsidering, faking fractional seconds is maybe not the best idea when we donâ€™t have to.
      # There are other options like event.sequence but even more simply,
      # we can just use event.created as tiebreaker.
      #
      # if exists(.event.created) {
      #   parsed_timestamp_regex, err = parse_regex(
      #     .message,
      #     r'^[^\s]+\s+(?P<pre>\d{4}-\d{2}-\d{2}[Tt\s]\d{2}:\d{2}:\d{2})(?:\.(?P<fract_secs>\d+))?(?P<tz>[Zz0-9:+-]+)\s'
      #   )
      #   if err != null {
      #     .__.parse_failures = push(.__.parse_failures, err)
      #   } else {
      #     parsed_created_timestamp_regex, err = parse_regex(
      #       .event.created,
      #       r'^\d{4}-\d{2}-\d{2}[Tt\s]\d{2}:\d{2}:\d{2}\.(?P<fract_secs>\d+)[Zz+-][0-9:-]+$'
      #     )
      #     if err != null {
      #       .__.parse_failures = push(.__.parse_failures, err)
      #     } else {
      #       orig_fract_secs = parsed_timestamp_regex.fract_secs || "0"
      #       orig_digits = length(orig_fract_secs)
      #       fill_digits = 0
      #       if orig_digits < 6 {
      #         fill_digits = 6 - orig_digits
      #       }
      #       faked_digits = 9 - orig_digits - fill_digits
      #       fract_secs = (slice(orig_fract_secs, 0, orig_digits) ?? "0") +
      #         (fill_digits * "0") +
      #         (slice(parsed_created_timestamp_regex.fract_secs, 0, faked_digits) ?? "")
      #       .timestamp = parsed_timestamp_regex.pre + "." + fract_secs + parsed_timestamp_regex.tz
      #       .event.timezone = parsed_timestamp_regex.tz
      #       # .x_ts = parsed_created_timestamp_regex
      #       # .x_fract_secs = fract_secs
      #       # .x_orig_digits = orig_digits
      #       # .x_faked_digits = faked_digits
      #       # ."x_@timestamp" = del(.timestamp)
      #     }
      #   }
      # }
      # ]]]

  transform_postprocess:
    type: remap
    inputs:
      - 'transform_hook_pre_postprocess'
    source: |-
      if (!exists(.__.parse_failures)) {
        # Component called from isolated unit test.
        .__.parse_failures_short = []
        .__.parse_failures = []
        .__.parse_warnings_short = []
        .__.parse_warnings = []
        .__.fingerprint_inputs = []
      }

      # log.file.path sanitation [[[
      # https://stackoverflow.com/questions/1589930/so-what-is-the-right-direction-of-the-paths-slash-or-under-windows/1589959#1589959
      if is_object(.__.enabled_postprocessors."file path sanitation") && exists(.log.file.path) {
        .log.file.path = replace!(.log.file.path, "\\", "/")
      }
      if is_object(.__.enabled_postprocessors."file path sanitation") && exists(.file.path) {
        .file.path = replace!(.file.path, "\\", "/")
      }
      # ]]]

      # host.name_rdns sanitation [[[
      # Spec: [host] is lowercase and does not include the domain part by
      # default if the domain would be the given domain. For all other domains,
      # this field should be the FQDN.
      if exists(.host.name_rdns) {
        .host.name_rdns = downcase(.host.name_rdns) ?? .host.name_rdns
        if (exists(.__.drop_given_domains_from_hostname)) {
          .host.name_rdns = replace(.host.name_rdns, .__.drop_given_domains_from_hostname, "") ?? .host.name_rdns
        }
      }
      if exists(.observer.name_rdns) {
        .observer.name_rdns = downcase(.observer.name_rdns) ?? .observer.name_rdns
        if (exists(.__.drop_given_domains_from_hostname)) {
          .observer.name_rdns = replace(.observer.name_rdns, .__.drop_given_domains_from_hostname, "") ?? .observer.name_rdns
        }
      }
      # ]]]

      # Handle empty/null host.name (Syslog message contains "-" for hostname) [[[
      if exists(.host.name) {
        if (.host.name == null) {
          if exists(.host.name_rdns) {
            # Disabled warning to not spam our log for not behaving switches.
            # .__.parse_warnings_short = push(.__.parse_warnings_short, "parse_warning: host.name missing")
            # .__.parse_warnings = push(.__.parse_warnings, "parse_warning: host.name missing. host.name is not contained in event.original. Using host.name_rdns.")

            .host.name = .host.name_rdns

            # It is required to include host.name_rdns in the fingerprint
            # calculation because otherwise events from different hosts could
            # get deduplicated.
            .__.fingerprint_inputs = push!(.__.fingerprint_inputs, .host.name_rdns)
          } else {
            .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: host.name* missing")
            .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: host.name* missing. Neither host.name nor host.name_rdns are known.")
            del(.host.name)
          }
        }
      }
      if exists(.observer.name) {
        if (.observer.name == null) {
          if exists(.observer.name_rdns) {
            # Disabled warning to not spam our log for not behaving switches.
            # .__.parse_warnings_short = push(.__.parse_warnings_short, "parse_warning: observer.name missing")
            # .__.parse_warnings = push(.__.parse_warnings, "parse_warning: observer.name missing. observer.name is not contained in event.original. Using observer.name_rdns.")

            .observer.name = .observer.name_rdns

            # This requires to include observer.name_rdns in the fingerprint
            # calculation because otherwise events from different hosts could
            # get deduplicated.
            .__.fingerprint_inputs = push!(.__.fingerprint_inputs, .observer.name_rdns)
          } else {
            .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: observer.name* missing")
            .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: observer.name* missing. Neither observer.name nor observer.name_rdns are known.")
            del(.observer.name)
          }
        }
      }
      # ]]]

      # host.name sanitation [[[
      if (exists(.host.name) && bool(.__.enabled_postprocessors."host name sanitation"."host.name") ?? false) {
        .host.name = downcase(.host.name) ?? .host.name
        if (exists(.__.drop_given_domains_from_hostname)) {
          .host.name = replace(.host.name, .__.drop_given_domains_from_hostname, "") ?? .host.name
        }
      }
      if (exists(.observer.name)) {
        .observer.name = downcase(.observer.name) ?? .observer.name
        if (exists(.__.drop_given_domains_from_hostname)) {
          .observer.name = replace(.observer.name, .__.drop_given_domains_from_hostname, "") ?? .observer.name
        }
      }
      if exists(.agent.hostname) {
        .agent.hostname = downcase!(.agent.hostname)
        if (exists(.__.drop_given_domains_from_hostname)) {
          .agent.hostname = replace(.agent.hostname, .__.drop_given_domains_from_hostname, "") ?? .agent.hostname
        }
      }
      # ]]]

      # host.name QA: Check against .host.name_rdns [[[
      # Use trusted host object that rsyslog generated based on reverse DNS
      # lookup of the IP address the event was sent from.
      if (.__.enabled_postprocessors."host name QA"."host.name" && exists(.host.name) ?? false) {
        if exists(.host.name_rdns) {
          # Is host.name the IP address of the log origin?
          # Some devices write their IP address into the syslog host field instead
          # of their hostname.
          if (exists(.host.ip) && .host.name == .host.ip) {
            .host.name = .host.name_rdns
          } else if (.host.name == .host.name_rdns) {
            del(.host.name_rdns)
          } else {
            .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: host.name QA")
            .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: host.name QA. host.name field is untrusted because it is different to host.name_rdns.")
          }
        } else {
          .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: host.name QA")
          .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: host.name QA. host.name field is untrusted because host.name_rdns is not available for cross checking.")
        }
      }
      if exists(.observer.name) {
        if exists(.observer.name_rdns) {
          if .observer.name == .observer.name_rdns {
            del(.observer.name_rdns)
          } else {
            .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: observer.name QA")
            .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: observer.name QA. observer.name field is untrusted because it is different to observer.name_rdns.")
          }
        } else {
          .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: observer.name QA")
          .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: observer.name QA. observer.name field is untrusted because observer.name_rdns is not available for cross checking.")
        }
      }
      # ]]]

      # @timestamp [[[
      if exists(."@timestamp") {
        # @timestamp QA: Use event.created if @timestamp is more a configurable duration off [[[
        if (
          !is_nullish(string(.__.enabled_postprocessors."timestamp QA"."@timestamp") ?? "") &&
          exists(.event.created)
        ) {
          event_created_parsed, err = parse_timestamp(.event.created, format: "%+")
          if err != null {
            .__.parse_failures_short = push!(.__.parse_failures_short, "parse_failure: event.created invalid")
            .__.parse_failures = push!(.__.parse_failures, "parse_failure: event.created invalid. " + err + ". Drop value: " + string!(.event.created))
            del(.event.created)
          } else {
            event_created_unix_ns = to_unix_timestamp(event_created_parsed, unit: "nanoseconds")
            # .__.parse_warnings = push(.__.parse_warnings, "parse_warning: @timestamp QA. QA not possible because event.created field could not be parsed as timestamp.")
            timestamp_parsed, err = parse_timestamp(."@timestamp", format: "%+")
            if err != null {
              .__.parse_failures_short = push!(.__.parse_failures_short, "parse_failure: @timestamp invalid")
              .__.parse_failures = push!(.__.parse_failures, "parse_failure: @timestamp invalid. " + err + ". Using event.created for the following bad @timestamp value: " + string!(."@timestamp"))
              ."@timestamp" = .event.created
            } else {
              timestamp_unix_ns = to_unix_timestamp(timestamp_parsed, unit: "nanoseconds")
              # .__.parse_warnings = push(.__.parse_warnings, "parse_warning: @timestamp QA. QA not possible because @timestamp field could not be parsed as timestamp.")
              timestamp_diff_ns = event_created_unix_ns - timestamp_unix_ns

              .event.created_delay = timestamp_diff_ns

              # No Math.abs exists in Vector.
              if (timestamp_diff_ns < 0) {
                timestamp_diff_ns = timestamp_diff_ns * -1;
              }

              if (timestamp_diff_ns > parse_duration!(.__.enabled_postprocessors."timestamp QA"."@timestamp", unit: "ns")) {
                .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: @timestamp QA")
                .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: @timestamp QA. @timestamp field is more than " + (string(.__.enabled_postprocessors."timestamp QA"."@timestamp") ?? "?") + " off from event.created. Using event.created as @timestamp.")
                ."@timestamp" = event_created_parsed
              }
            }
          }
        }
        # ]]]
      } else {
        if exists(.event.created) {
          .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: @timestamp missing")
          # TODO: Better error message: No original @timestamp
          .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: @timestamp missing. No time stamp exists describing when the event occurred. Falling back to event.created.")
          ."@timestamp" = .event.created
        } else {
          # TODO: Better error message: No original @timestamp
          .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: @timestamp missing")
          .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: @timestamp missing. No time stamp exists describing when the event occurred and event.created does not exist either. Falling back to timestamp when the event was processed by the log collection pipeline.")
          ."@timestamp" = now()
        }
      }
      # ]]]

      # Calculate event.duration from event.start and event.end [[[
      if !exists(.event.duration) && exists(.event.start) && exists(.event.end) {
        event_start_ns, err = to_unix_timestamp(.event.start, unit: "nanoseconds")
        if err != null {
          .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: to_unix_timestamp")
          .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: to_unix_timestamp. Cannot calculate event.duration because event.start cannot be parsed: " + err)
          } else {
          event_end_ns, err = to_unix_timestamp(.event.end, unit: "nanoseconds")
          if err != null {
            .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: to_unix_timestamp")
            .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: to_unix_timestamp. Cannot calculate event.duration because event.end cannot be parsed: " + err)
          } else {
            .event.duration = event_end_ns - event_start_ns
          }
        }
      }
      # ]]]

      # Derive log.level from event.severity [[[
      if (!exists(.log.level) && exists(.event.severity)) {
        .log.level = to_syslog_level!(.event.severity)
      }
      # ]]]

      # Derive log.file.name from log.file.path [[[
      if (!exists(.log.file.name) && is_string(.log.file.path)) {
        .log.file.name = replace!(.log.file.path, r'.*/', "")
      }
      # ]]]

      # Set fallback event.original [[[
      # We do this as fallback and not before to avoid having the
      # event.original field end up in fingerprint_inputs.
      # Consider refactoring this. I stopped doing that because it is difficult
      # with the two decode JSON steps.
      if (!exists(.event.original)) {
        if (!exists(.event.original.message)) {
          .event.original = .__.event.original.message
        } else {
          .event.original = .__.event.original
        }
        .__.fingerprint_inputs = [.event.original]
      }
      # ]]]

      if (!is_nullish(.message)) {
        .message = strip_whitespace(.message) ?? .message
      }

      # The event.ingested field should really be set at the latest possible
      # moment. Otherwise you risk it being wrong. For example when the Vector
      # sink has to queue events because the service the sink is writing to is
      # not available. If you cannot set the field later in the event
      # collection pipeline, set it in the transform_final component. The
      # field is not set by default by the framework to make it easy to spot
      # its absence.
      # Examples where it should be set: Elasticsearch ingest pipelines.
      # .event.ingested = now()

  # This transform is used on events after they are parsed in Vector.
  # If the event is to be processed by Elastic ingest pipelines, this transform
  # is run before the main parsing step instead. This is because of the vendor
  # lock-in of ingest pipelines (it is not really possible to continue
  # processing an event after it entered the ingest pipeline).
  transform_postprocess_all:
    type: remap
    inputs:
      - 'transform_postprocess'
      # - 'transform_pre_ingest_pipeline_module_*'
    source: |-
      # Calculate deterministic fingerprint and document ID [[[
      # Ref: https://www.elastic.co/blog/logstash-lessons-handling-duplicates
      # This needs to be done as early as possible against the raw event to
      # allow changes in the event transformation without altering the ID. SHA2 is
      # chosen because it is considered cryptographically secure (other than
      # SHA1 which is broken). This is also important because otherwise,
      # someone who could potentially do a second-preimage attack to
      # overwrite/change any log in ES that she does not like and of which she
      # has the original version (log line). SHA512 is chosen over SHA256
      # because SHA512 is considered faster on modern CPUs. MD5 was considered
      # because it is slightly faster according to `openssl speed md5 sha512`.
      #
      # The key of the HMAC can be used to proof the authenticity of the
      # source fields as a side effect.
      # Note that HMAC is an authentication code and not an "encryption" even though we have a "key" defined.
      # Format version history:
      # v3 and lower: Logstash.
      # v4: Raw .message (mostly encoded JSON).
      # v5: Raw .message or decoded JSON fileds .message and .host.name_rdns (or .observer.name_rdns).
      #     Reason: To support deduplicating the same log lines. This was not possible before raw .message included event.created.
      #             .host.name is included because some syslog senders do not
      #             include their hostname so without this it could happen that
      #             events from different hosts get deduplicated into one.
      if !exists(.__._id) {
        if !exists(.__.event.hash) && exists(.event.original) {
          .__.fingerprint_inputs = push!(.__.fingerprint_inputs, .__.hmac_secret_key)
          .__.event.hash = sha2(encode_json(.__.fingerprint_inputs), variant: "SHA-512/256")
        }
        .__._id = "v5_" + slice(.__.event.hash, 0, 23) ?? .__.event.hash
      }
      # ]]]

      # Ensure agent.name is set [[[
      if (exists(.agent.name)) {
        if (is_string(.agent.name)) {
          .agent.name = [.agent.name]
        } else if (!is_array(.agent.name)) {
          .__.parse_warnings_short = push!(.__.parse_warnings_short, "parse_warning: agent.name wrong type")
          .__.parse_warnings = push!(.__.parse_warnings, "parse_warning: agent.name wrong type. Resetting to empty array. Previous Value: " + (to_string(.agent.name) ?? "[unknown]"))
          .agent.name = []
        }
      } else {
        .agent.name = []
      }
      if ((get_env_var("TEST_MODE") ?? false) == "true") {
        # TODO: Remove this unneeded condition. Just set VECTOR_HOSTNAME in the test case.
        .agent.name = push!(.agent.name, "fixed-test-host-name")
      } else {
        .agent.name = push!(.agent.name, downcase(get_env_var("VECTOR_HOSTNAME") ?? get_hostname!()))
      }
      # ]]]

      # source_type should not be sent by agents. Ensure that it is absent until all agent and archive files are without source_type.
      # TODO: Still needed here? We already do this in transform_prepare_event_metadata
      del(.source_type)

      # Put together .tags [[[
      if (length(.__.parse_failures) != 0 ?? false) {
        # Allow indexed search for documents with one ore more parse_failure.
        .__.parse_failures = push!(.__.parse_failures, "parse_failure")

        if ! is_array(.tags) { .tags = [] }
        .tags = append!(.tags, .__.parse_failures_short)
        if ! is_array(.log.flags) { .log.flags = [] }
        .log.flags = append!(.log.flags, .__.parse_failures)
      }
      if is_array(.__.parse_warnings) && length!(.__.parse_warnings) != 0 {
        # Allow indexed search for documents with one or more parse_warning.
        .__.parse_warnings = push!(.__.parse_warnings, "parse_warning")

        if ! is_array(.tags) { .tags = [] }
        .tags = append(.tags, .__.parse_warnings_short) ?? ["parse_warning: tag append failed"]
        if ! is_array(.log.flags) { .log.flags = [] }
        .log.flags = append!(.log.flags, .__.parse_warnings)
      }
      # ]]]

      . = compact(., string: false)
